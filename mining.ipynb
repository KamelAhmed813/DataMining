{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed packages for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests and BeautifulSoup -> for sending HTTP requests and parsing the respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas and numpy -> for data engineering and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle -> for saving the data locally from the RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os -> for handling local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Mining function which takes a list of URLs and Map variable to return data in it.\n",
    "It parses data based on Aqarmap html files only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mining(links, houses_data):\n",
    "    '''\n",
    "    links -> a list of URLs to send a HTTP request and parse the HTML response of it\n",
    "    houses_data -> Empty map where data is returned and item for each link with the area as the Key and the list of found properties are the value\n",
    "    '''\n",
    "    for link in links:\n",
    "        if(link['area'] not in houses_data.keys()):\n",
    "            houses_data[link['area']] = []\n",
    "        i=1\n",
    "        while(len(houses_data[link['area']])<link['number_of_appartments']):\n",
    "            url = f\"{link['link']}?page={i}\"\n",
    "            request = requests.get(url)\n",
    "            soup = BeautifulSoup(request.content, 'html.parser')\n",
    "            data = soup.find_all('div', class_=\"search-listing-card\")\n",
    "            for h in data:\n",
    "                id = h['id']\n",
    "                title = h.find('h2')\n",
    "                title = title.text.strip() if title is not None else ''\n",
    "                price = h.find('div', class_=\"search-listing-card__price\").find('span')\n",
    "                price = price.text.strip() if price is not None else 0\n",
    "                location = h.find('p', class_=\"search-listing-card__address\").find('span')\n",
    "                location = location.text if location is not None else ''\n",
    "                attributes = h.find('div', class_=\"search-listing-card__attributes\").find_all('label')\n",
    "                attrib = []\n",
    "                for a in attributes:\n",
    "                    attrib.append(a.text.strip())\n",
    "                size = attrib[0].split(' ') if len(attrib) > 0 else 0\n",
    "                bedrooms = attrib[1] if len(attrib) > 1 else 1\n",
    "                bathrooms = attrib[2] if len(attrib) > 2 else 1\n",
    "                houses_data[link['area']].append({\n",
    "                                    'id':id,\n",
    "                                    'title':title,\n",
    "                                    'price':price,\n",
    "                                    'location':location,\n",
    "                                    'size':size[0],\n",
    "                                    'size_unit':size[1],\n",
    "                                    'bedrooms':bedrooms,\n",
    "                                    'bathrooms':bathrooms,\n",
    "                                    })\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting list of URLs of Alexandria properties based on Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_location = 'cairo/'\n",
    "rent_of_sale = 'for-rent/'\n",
    "'''\n",
    "rent -> for-rent/,\n",
    "sale -> for-sale/\n",
    "'''\n",
    "type_of_property = 'apartment/'\n",
    "'''\n",
    "apartment -> apartment/,\n",
    "villa -> villa/,\n",
    "furnished-apartment -> furnished-apartment/,\n",
    "all -> property-type/\n",
    "'''\n",
    "website = 'https://aqarmap.com.eg/ar/'\n",
    "request = requests.get(website+rent_of_sale+type_of_property+search_location)\n",
    "soup = BeautifulSoup(request.content, 'html.parser')\n",
    "data = soup.find('div', class_='sectionLoadMore').find_all('li')\n",
    "\n",
    "Alexandria_appertmints_links = []\n",
    "for l in data:\n",
    "    loca = l.find('a')\n",
    "    title = loca.text.split('(')\n",
    "    Alexandria_appertmints_links.append({\n",
    "                                        'area':title[0].strip(),\n",
    "                                        'number_of_appartments':int(title[1].split(')')[0]),\n",
    "                                        'link':f\"https://aqarmap.com.eg{loca['href']}\",\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in Alexandria_appertmints_links:\n",
    "    if('area' in area.keys()):\n",
    "        print(area['area'] + \"\\t\" + str(area['number_of_appartments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alex_data = {}\n",
    "data_mining(Alexandria_appertmints_links, Alex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alex_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Dataframes and saving them to separate csv files for each location area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in Alex_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f'Alex_properties\\\\{key}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading all the csv files and building a unified file containing all the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Alex_properties//أبو تلات.csv')\n",
    "df['location_region'] = 'أبو تلات'\n",
    "test=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.curdir,'Alex_properties')\n",
    "for f in os.listdir(path):\n",
    "    if test:\n",
    "        test=False\n",
    "        continue\n",
    "    else:\n",
    "        df2 = pd.read_csv(os.path.join(path, f))\n",
    "        df2['location_region'] = f.split('.')[0]\n",
    "        df = pd.concat([df, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('All_Alex_Properties.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "710066b28cf7dcd4b5e5b98767647a294315a835e70e68ead878e1e5379e5ebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
